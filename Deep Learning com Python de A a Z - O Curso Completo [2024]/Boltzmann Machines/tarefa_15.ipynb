{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Library Versions\n",
    "np.__version__, matplotlib.__version__, sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.26.4', '3.8.4', '1.4.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display versions of the imported libraries\n",
    "np.__version__, matplotlib.__version__, sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "previsores = np.asarray(digits.data, 'float32')\n",
    "classe = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "previsores = np.asarray(digits.data, 'float32')\n",
    "classe = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "normalizador = MinMaxScaler(feature_range = (0, 1))\n",
    "previsores = normalizador.fit_transform(previsores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize data to the range between 0 and 1\n",
    "normalizador = MinMaxScaler(feature_range=(0, 1))\n",
    "previsores = normalizador.fit_transform(previsores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training and Testing Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Configure Restricted Boltzmann Machine (RBM)\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "\n",
    "rbm = BernoulliRBM(random_state=0)\n",
    "rbm.n_iter = 25\n",
    "rbm.n_components = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "\n",
    "# Create and configure Restricted Boltzmann Machine (RBM)\n",
    "rbm = BernoulliRBM(random_state=0)\n",
    "rbm.n_iter = 25\n",
    "rbm.n_components = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Configure Neural Network (MLP)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_rbm = MLPClassifier(hidden_layer_sizes=(37, 37),\n",
    "                        activation='relu',\n",
    "                        solver='adam',\n",
    "                        batch_size=50,\n",
    "                        max_iter=1000,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Create and configure the neural network using scikit-learn\n",
    "mlp_rbm = MLPClassifier(hidden_layer_sizes=(37, 37),\n",
    "                        activation='relu',\n",
    "                        solver='adam',\n",
    "                        batch_size=50,\n",
    "                        max_iter=1000,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pipeline and Train Model with RBM + MLP\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "classificador_rbm = Pipeline(steps=[('rbm', rbm), ('mlp', mlp_rbm)])\n",
    "classificador_rbm.fit(previsores_treinamento, classe_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.21321123\n",
      "Iteration 2, loss = 1.97307739\n",
      "Iteration 3, loss = 1.64887942\n",
      "Iteration 4, loss = 1.24312452\n",
      "Iteration 5, loss = 0.89401240\n",
      "Iteration 6, loss = 0.67573258\n",
      "Iteration 7, loss = 0.56191103\n",
      "Iteration 8, loss = 0.48040406\n",
      "Iteration 9, loss = 0.42802553\n",
      "Iteration 10, loss = 0.38656970\n",
      "Iteration 11, loss = 0.35669465\n",
      "Iteration 12, loss = 0.33134565\n",
      "Iteration 13, loss = 0.31477165\n",
      "Iteration 14, loss = 0.29970725\n",
      "Iteration 15, loss = 0.28479921\n",
      "Iteration 16, loss = 0.27788571\n",
      "Iteration 17, loss = 0.26721086\n",
      "Iteration 18, loss = 0.26075554\n",
      "Iteration 19, loss = 0.25430523\n",
      "Iteration 20, loss = 0.24438797\n",
      "Iteration 21, loss = 0.23741333\n",
      "Iteration 22, loss = 0.23244175\n",
      "Iteration 23, loss = 0.22953760\n",
      "Iteration 24, loss = 0.22513262\n",
      "Iteration 25, loss = 0.21899246\n",
      "Iteration 26, loss = 0.21369281\n",
      "Iteration 27, loss = 0.21144843\n",
      "Iteration 28, loss = 0.20742634\n",
      "Iteration 29, loss = 0.20641955\n",
      "Iteration 30, loss = 0.20162107\n",
      "Iteration 31, loss = 0.19814463\n",
      "Iteration 32, loss = 0.19415619\n",
      "Iteration 33, loss = 0.19400386\n",
      "Iteration 34, loss = 0.18917686\n",
      "Iteration 35, loss = 0.18611954\n",
      "Iteration 36, loss = 0.18511285\n",
      "Iteration 37, loss = 0.18150795\n",
      "Iteration 38, loss = 0.17921266\n",
      "Iteration 39, loss = 0.17875710\n",
      "Iteration 40, loss = 0.17488767\n",
      "Iteration 41, loss = 0.17546307\n",
      "Iteration 42, loss = 0.17466977\n",
      "Iteration 43, loss = 0.17347160\n",
      "Iteration 44, loss = 0.16746257\n",
      "Iteration 45, loss = 0.16532381\n",
      "Iteration 46, loss = 0.16763153\n",
      "Iteration 47, loss = 0.16334532\n",
      "Iteration 48, loss = 0.16009518\n",
      "Iteration 49, loss = 0.16173873\n",
      "Iteration 50, loss = 0.15833728\n",
      "Iteration 51, loss = 0.15652387\n",
      "Iteration 52, loss = 0.15565576\n",
      "Iteration 53, loss = 0.15171854\n",
      "Iteration 54, loss = 0.15181331\n",
      "Iteration 55, loss = 0.14946943\n",
      "Iteration 56, loss = 0.14785637\n",
      "Iteration 57, loss = 0.14699052\n",
      "Iteration 58, loss = 0.14894760\n",
      "Iteration 59, loss = 0.14564193\n",
      "Iteration 60, loss = 0.14633648\n",
      "Iteration 61, loss = 0.14298075\n",
      "Iteration 62, loss = 0.14088520\n",
      "Iteration 63, loss = 0.13672349\n",
      "Iteration 64, loss = 0.13691465\n",
      "Iteration 65, loss = 0.13964162\n",
      "Iteration 66, loss = 0.13547543\n",
      "Iteration 67, loss = 0.13628799\n",
      "Iteration 68, loss = 0.13585427\n",
      "Iteration 69, loss = 0.13166198\n",
      "Iteration 70, loss = 0.13068603\n",
      "Iteration 71, loss = 0.13080334\n",
      "Iteration 72, loss = 0.13184573\n",
      "Iteration 73, loss = 0.13185174\n",
      "Iteration 74, loss = 0.12926801\n",
      "Iteration 75, loss = 0.12509949\n",
      "Iteration 76, loss = 0.12447154\n",
      "Iteration 77, loss = 0.12611010\n",
      "Iteration 78, loss = 0.12668745\n",
      "Iteration 79, loss = 0.12310065\n",
      "Iteration 80, loss = 0.12254499\n",
      "Iteration 81, loss = 0.12200139\n",
      "Iteration 82, loss = 0.11982367\n",
      "Iteration 83, loss = 0.11856471\n",
      "Iteration 84, loss = 0.11727403\n",
      "Iteration 85, loss = 0.11559645\n",
      "Iteration 86, loss = 0.11972419\n",
      "Iteration 87, loss = 0.11720423\n",
      "Iteration 88, loss = 0.11773811\n",
      "Iteration 89, loss = 0.11697202\n",
      "Iteration 90, loss = 0.11541423\n",
      "Iteration 91, loss = 0.11742408\n",
      "Iteration 92, loss = 0.11612591\n",
      "Iteration 93, loss = 0.11130373\n",
      "Iteration 94, loss = 0.11484972\n",
      "Iteration 95, loss = 0.11087315\n",
      "Iteration 96, loss = 0.10817075\n",
      "Iteration 97, loss = 0.11033822\n",
      "Iteration 98, loss = 0.10731343\n",
      "Iteration 99, loss = 0.10723292\n",
      "Iteration 100, loss = 0.10590528\n",
      "Iteration 101, loss = 0.10487391\n",
      "Iteration 102, loss = 0.10588963\n",
      "Iteration 103, loss = 0.10381399\n",
      "Iteration 104, loss = 0.10228803\n",
      "Iteration 105, loss = 0.10381749\n",
      "Iteration 106, loss = 0.10204875\n",
      "Iteration 107, loss = 0.10492225\n",
      "Iteration 108, loss = 0.10065992\n",
      "Iteration 109, loss = 0.10306700\n",
      "Iteration 110, loss = 0.09955572\n",
      "Iteration 111, loss = 0.10257239\n",
      "Iteration 112, loss = 0.09895816\n",
      "Iteration 113, loss = 0.09942852\n",
      "Iteration 114, loss = 0.09871178\n",
      "Iteration 115, loss = 0.09479228\n",
      "Iteration 116, loss = 0.09568518\n",
      "Iteration 117, loss = 0.09597002\n",
      "Iteration 118, loss = 0.09552791\n",
      "Iteration 119, loss = 0.09570570\n",
      "Iteration 120, loss = 0.09326541\n",
      "Iteration 121, loss = 0.09555634\n",
      "Iteration 122, loss = 0.09609037\n",
      "Iteration 123, loss = 0.09323110\n",
      "Iteration 124, loss = 0.09023713\n",
      "Iteration 125, loss = 0.08895761\n",
      "Iteration 126, loss = 0.08883636\n",
      "Iteration 127, loss = 0.09592526\n",
      "Iteration 128, loss = 0.09208611\n",
      "Iteration 129, loss = 0.09289780\n",
      "Iteration 130, loss = 0.09216581\n",
      "Iteration 131, loss = 0.09160568\n",
      "Iteration 132, loss = 0.08506078\n",
      "Iteration 133, loss = 0.08630998\n",
      "Iteration 134, loss = 0.09054594\n",
      "Iteration 135, loss = 0.08604985\n",
      "Iteration 136, loss = 0.08466830\n",
      "Iteration 137, loss = 0.08372673\n",
      "Iteration 138, loss = 0.08766067\n",
      "Iteration 139, loss = 0.08309472\n",
      "Iteration 140, loss = 0.08328722\n",
      "Iteration 141, loss = 0.08478856\n",
      "Iteration 142, loss = 0.08276544\n",
      "Iteration 143, loss = 0.08072473\n",
      "Iteration 144, loss = 0.08241305\n",
      "Iteration 145, loss = 0.08424713\n",
      "Iteration 146, loss = 0.08001198\n",
      "Iteration 147, loss = 0.07890581\n",
      "Iteration 148, loss = 0.07883638\n",
      "Iteration 149, loss = 0.07790220\n",
      "Iteration 150, loss = 0.07938878\n",
      "Iteration 151, loss = 0.07969428\n",
      "Iteration 152, loss = 0.07703863\n",
      "Iteration 153, loss = 0.07770718\n",
      "Iteration 154, loss = 0.07682987\n",
      "Iteration 155, loss = 0.07664254\n",
      "Iteration 156, loss = 0.07593735\n",
      "Iteration 157, loss = 0.07596559\n",
      "Iteration 158, loss = 0.07395859\n",
      "Iteration 159, loss = 0.07319805\n",
      "Iteration 160, loss = 0.07396267\n",
      "Iteration 161, loss = 0.07484132\n",
      "Iteration 162, loss = 0.07398652\n",
      "Iteration 163, loss = 0.07054550\n",
      "Iteration 164, loss = 0.07198878\n",
      "Iteration 165, loss = 0.07222077\n",
      "Iteration 166, loss = 0.07202516\n",
      "Iteration 167, loss = 0.07425947\n",
      "Iteration 168, loss = 0.07231291\n",
      "Iteration 169, loss = 0.07114895\n",
      "Iteration 170, loss = 0.06805193\n",
      "Iteration 171, loss = 0.07433512\n",
      "Iteration 172, loss = 0.07100824\n",
      "Iteration 173, loss = 0.06791770\n",
      "Iteration 174, loss = 0.06933509\n",
      "Iteration 175, loss = 0.06660794\n",
      "Iteration 176, loss = 0.06732382\n",
      "Iteration 177, loss = 0.07235519\n",
      "Iteration 178, loss = 0.06697021\n",
      "Iteration 179, loss = 0.06649334\n",
      "Iteration 180, loss = 0.06551411\n",
      "Iteration 181, loss = 0.06464698\n",
      "Iteration 182, loss = 0.06622992\n",
      "Iteration 183, loss = 0.06658080\n",
      "Iteration 184, loss = 0.06455089\n",
      "Iteration 185, loss = 0.06274717\n",
      "Iteration 186, loss = 0.06265100\n",
      "Iteration 187, loss = 0.06202951\n",
      "Iteration 188, loss = 0.06231530\n",
      "Iteration 189, loss = 0.06274648\n",
      "Iteration 190, loss = 0.06372385\n",
      "Iteration 191, loss = 0.06308468\n",
      "Iteration 192, loss = 0.06002760\n",
      "Iteration 193, loss = 0.06061771\n",
      "Iteration 194, loss = 0.06048929\n",
      "Iteration 195, loss = 0.05784679\n",
      "Iteration 196, loss = 0.05924348\n",
      "Iteration 197, loss = 0.05770695\n",
      "Iteration 198, loss = 0.05723398\n",
      "Iteration 199, loss = 0.05822657\n",
      "Iteration 200, loss = 0.05814336\n",
      "Iteration 201, loss = 0.05807478\n",
      "Iteration 202, loss = 0.05947379\n",
      "Iteration 203, loss = 0.06047372\n",
      "Iteration 204, loss = 0.05804577\n",
      "Iteration 205, loss = 0.05459669\n",
      "Iteration 206, loss = 0.05587564\n",
      "Iteration 207, loss = 0.05677495\n",
      "Iteration 208, loss = 0.05482340\n",
      "Iteration 209, loss = 0.05414880\n",
      "Iteration 210, loss = 0.05227721\n",
      "Iteration 211, loss = 0.05321984\n",
      "Iteration 212, loss = 0.05353334\n",
      "Iteration 213, loss = 0.05318232\n",
      "Iteration 214, loss = 0.05551919\n",
      "Iteration 215, loss = 0.05069510\n",
      "Iteration 216, loss = 0.05351830\n",
      "Iteration 217, loss = 0.05470007\n",
      "Iteration 218, loss = 0.05087509\n",
      "Iteration 219, loss = 0.05358658\n",
      "Iteration 220, loss = 0.05085639\n",
      "Iteration 221, loss = 0.05008674\n",
      "Iteration 222, loss = 0.05041458\n",
      "Iteration 223, loss = 0.05081120\n",
      "Iteration 224, loss = 0.05244127\n",
      "Iteration 225, loss = 0.05499663\n",
      "Iteration 226, loss = 0.05178862\n",
      "Iteration 227, loss = 0.05037343\n",
      "Iteration 228, loss = 0.04945371\n",
      "Iteration 229, loss = 0.04750593\n",
      "Iteration 230, loss = 0.04850249\n",
      "Iteration 231, loss = 0.04743773\n",
      "Iteration 232, loss = 0.04797013\n",
      "Iteration 233, loss = 0.04967753\n",
      "Iteration 234, loss = 0.04724418\n",
      "Iteration 235, loss = 0.04723156\n",
      "Iteration 236, loss = 0.04522532\n",
      "Iteration 237, loss = 0.04708057\n",
      "Iteration 238, loss = 0.04574893\n",
      "Iteration 239, loss = 0.04439546\n",
      "Iteration 240, loss = 0.04683700\n",
      "Iteration 241, loss = 0.04566756\n",
      "Iteration 242, loss = 0.04596920\n",
      "Iteration 243, loss = 0.04919319\n",
      "Iteration 244, loss = 0.04424974\n",
      "Iteration 245, loss = 0.04652296\n",
      "Iteration 246, loss = 0.04612458\n",
      "Iteration 247, loss = 0.04335522\n",
      "Iteration 248, loss = 0.04665205\n",
      "Iteration 249, loss = 0.04272321\n",
      "Iteration 250, loss = 0.04411551\n",
      "Iteration 251, loss = 0.04417251\n",
      "Iteration 252, loss = 0.04414411\n",
      "Iteration 253, loss = 0.04122980\n",
      "Iteration 254, loss = 0.04128644\n",
      "Iteration 255, loss = 0.04506404\n",
      "Iteration 256, loss = 0.04239648\n",
      "Iteration 257, loss = 0.04160851\n",
      "Iteration 258, loss = 0.04125048\n",
      "Iteration 259, loss = 0.04147315\n",
      "Iteration 260, loss = 0.04150362\n",
      "Iteration 261, loss = 0.04047136\n",
      "Iteration 262, loss = 0.03984843\n",
      "Iteration 263, loss = 0.03969358\n",
      "Iteration 264, loss = 0.03968028\n",
      "Iteration 265, loss = 0.03913988\n",
      "Iteration 266, loss = 0.03835251\n",
      "Iteration 267, loss = 0.03794832\n",
      "Iteration 268, loss = 0.03896608\n",
      "Iteration 269, loss = 0.03852920\n",
      "Iteration 270, loss = 0.04016573\n",
      "Iteration 271, loss = 0.04289029\n",
      "Iteration 272, loss = 0.03796433\n",
      "Iteration 273, loss = 0.04007271\n",
      "Iteration 274, loss = 0.04036127\n",
      "Iteration 275, loss = 0.03801782\n",
      "Iteration 276, loss = 0.03631622\n",
      "Iteration 277, loss = 0.03632714\n",
      "Iteration 278, loss = 0.03815001\n",
      "Iteration 279, loss = 0.03911937\n",
      "Iteration 280, loss = 0.03593308\n",
      "Iteration 281, loss = 0.03672726\n",
      "Iteration 282, loss = 0.03522194\n",
      "Iteration 283, loss = 0.03693594\n",
      "Iteration 284, loss = 0.03718389\n",
      "Iteration 285, loss = 0.03562136\n",
      "Iteration 286, loss = 0.03448238\n",
      "Iteration 287, loss = 0.03567402\n",
      "Iteration 288, loss = 0.03500920\n",
      "Iteration 289, loss = 0.03348587\n",
      "Iteration 290, loss = 0.03406092\n",
      "Iteration 291, loss = 0.03368237\n",
      "Iteration 292, loss = 0.03360272\n",
      "Iteration 293, loss = 0.03676070\n",
      "Iteration 294, loss = 0.03573391\n",
      "Iteration 295, loss = 0.03628099\n",
      "Iteration 296, loss = 0.03409609\n",
      "Iteration 297, loss = 0.03310941\n",
      "Iteration 298, loss = 0.03307700\n",
      "Iteration 299, loss = 0.03300973\n",
      "Iteration 300, loss = 0.03567391\n",
      "Iteration 301, loss = 0.03292124\n",
      "Iteration 302, loss = 0.03343279\n",
      "Iteration 303, loss = 0.03351456\n",
      "Iteration 304, loss = 0.03173413\n",
      "Iteration 305, loss = 0.03194269\n",
      "Iteration 306, loss = 0.03212439\n",
      "Iteration 307, loss = 0.03197215\n",
      "Iteration 308, loss = 0.03232239\n",
      "Iteration 309, loss = 0.03210140\n",
      "Iteration 310, loss = 0.03079759\n",
      "Iteration 311, loss = 0.03261276\n",
      "Iteration 312, loss = 0.02966236\n",
      "Iteration 313, loss = 0.03058063\n",
      "Iteration 314, loss = 0.03124766\n",
      "Iteration 315, loss = 0.03221250\n",
      "Iteration 316, loss = 0.03196589\n",
      "Iteration 317, loss = 0.02957439\n",
      "Iteration 318, loss = 0.03025486\n",
      "Iteration 319, loss = 0.03101605\n",
      "Iteration 320, loss = 0.02915218\n",
      "Iteration 321, loss = 0.02951348\n",
      "Iteration 322, loss = 0.03243850\n",
      "Iteration 323, loss = 0.02930771\n",
      "Iteration 324, loss = 0.02842474\n",
      "Iteration 325, loss = 0.02860568\n",
      "Iteration 326, loss = 0.02880754\n",
      "Iteration 327, loss = 0.03018340\n",
      "Iteration 328, loss = 0.03128653\n",
      "Iteration 329, loss = 0.02781693\n",
      "Iteration 330, loss = 0.02912423\n",
      "Iteration 331, loss = 0.02842212\n",
      "Iteration 332, loss = 0.02780072\n",
      "Iteration 333, loss = 0.03588678\n",
      "Iteration 334, loss = 0.02889193\n",
      "Iteration 335, loss = 0.02784598\n",
      "Iteration 336, loss = 0.02926323\n",
      "Iteration 337, loss = 0.03028651\n",
      "Iteration 338, loss = 0.02745449\n",
      "Iteration 339, loss = 0.02685777\n",
      "Iteration 340, loss = 0.02693361\n",
      "Iteration 341, loss = 0.02484466\n",
      "Iteration 342, loss = 0.02764855\n",
      "Iteration 343, loss = 0.02767293\n",
      "Iteration 344, loss = 0.02821992\n",
      "Iteration 345, loss = 0.02637427\n",
      "Iteration 346, loss = 0.02708819\n",
      "Iteration 347, loss = 0.02836414\n",
      "Iteration 348, loss = 0.02573943\n",
      "Iteration 349, loss = 0.02718246\n",
      "Iteration 350, loss = 0.02655540\n",
      "Iteration 351, loss = 0.02532001\n",
      "Iteration 352, loss = 0.02539423\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;rbm&#x27;,\n",
       "                 BernoulliRBM(n_components=50, n_iter=25, random_state=0)),\n",
       "                (&#x27;mlp&#x27;,\n",
       "                 MLPClassifier(batch_size=50, hidden_layer_sizes=(37, 37),\n",
       "                               max_iter=1000, verbose=1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;rbm&#x27;,\n",
       "                 BernoulliRBM(n_components=50, n_iter=25, random_state=0)),\n",
       "                (&#x27;mlp&#x27;,\n",
       "                 MLPClassifier(batch_size=50, hidden_layer_sizes=(37, 37),\n",
       "                               max_iter=1000, verbose=1))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;BernoulliRBM<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.BernoulliRBM.html\">?<span>Documentation for BernoulliRBM</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>BernoulliRBM(n_components=50, n_iter=25, random_state=0)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(batch_size=50, hidden_layer_sizes=(37, 37), max_iter=1000,\n",
       "              verbose=1)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('rbm',\n",
       "                 BernoulliRBM(n_components=50, n_iter=25, random_state=0)),\n",
       "                ('mlp',\n",
       "                 MLPClassifier(batch_size=50, hidden_layer_sizes=(37, 37),\n",
       "                               max_iter=1000, verbose=1))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create pipeline to execute RBM followed by MLP\n",
    "classificador_rbm = Pipeline(steps=[('rbm', rbm), ('mlp', mlp_rbm)])\n",
    "\n",
    "# Train the model with training data\n",
    "classificador_rbm.fit(previsores_treinamento, classe_treinamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions and Evaluate RBM + MLP Model\n",
    "from sklearn import metrics\n",
    "\n",
    "previsoes_rbm = classificador_rbm.predict(previsores_teste)\n",
    "precisao_rbm = metrics.accuracy_score(previsoes_rbm, classe_teste)\n",
    "precisao_rbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9472222222222222"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Make predictions using RBM + MLP model\n",
    "previsoes_rbm = classificador_rbm.predict(previsores_teste)\n",
    "\n",
    "# Evaluate the accuracy of the RBM + MLP model\n",
    "precisao_rbm = metrics.accuracy_score(previsoes_rbm, classe_teste)\n",
    "precisao_rbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Train Simple Neural Network (MLP)\n",
    "mlp_simples = MLPClassifier(hidden_layer_sizes=(37, 37),\n",
    "                        activation='relu',\n",
    "                        solver='adam',\n",
    "                        batch_size=50,\n",
    "                        max_iter=1000,\n",
    "                        verbose=1)\n",
    "\n",
    "mlp_simples.fit(previsores_treinamento, classe_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.23057159\n",
      "Iteration 2, loss = 1.97667160\n",
      "Iteration 3, loss = 1.56796523\n",
      "Iteration 4, loss = 1.10271232\n",
      "Iteration 5, loss = 0.73611865\n",
      "Iteration 6, loss = 0.52522467\n",
      "Iteration 7, loss = 0.41244889\n",
      "Iteration 8, loss = 0.34194194\n",
      "Iteration 9, loss = 0.29704369\n",
      "Iteration 10, loss = 0.25852293\n",
      "Iteration 11, loss = 0.23339485\n",
      "Iteration 12, loss = 0.20957272\n",
      "Iteration 13, loss = 0.19243860\n",
      "Iteration 14, loss = 0.17269078\n",
      "Iteration 15, loss = 0.16088966\n",
      "Iteration 16, loss = 0.15290105\n",
      "Iteration 17, loss = 0.13829895\n",
      "Iteration 18, loss = 0.12906245\n",
      "Iteration 19, loss = 0.11834769\n",
      "Iteration 20, loss = 0.11247994\n",
      "Iteration 21, loss = 0.10553994\n",
      "Iteration 22, loss = 0.10263387\n",
      "Iteration 23, loss = 0.09585294\n",
      "Iteration 24, loss = 0.08927231\n",
      "Iteration 25, loss = 0.08568561\n",
      "Iteration 26, loss = 0.08197550\n",
      "Iteration 27, loss = 0.07820931\n",
      "Iteration 28, loss = 0.07446759\n",
      "Iteration 29, loss = 0.07043276\n",
      "Iteration 30, loss = 0.06765959\n",
      "Iteration 31, loss = 0.06465959\n",
      "Iteration 32, loss = 0.06311838\n",
      "Iteration 33, loss = 0.05871571\n",
      "Iteration 34, loss = 0.05648938\n",
      "Iteration 35, loss = 0.05315633\n",
      "Iteration 36, loss = 0.05295330\n",
      "Iteration 37, loss = 0.04800396\n",
      "Iteration 38, loss = 0.04729883\n",
      "Iteration 39, loss = 0.04492087\n",
      "Iteration 40, loss = 0.04305450\n",
      "Iteration 41, loss = 0.04176219\n",
      "Iteration 42, loss = 0.04124860\n",
      "Iteration 43, loss = 0.03767174\n",
      "Iteration 44, loss = 0.03654106\n",
      "Iteration 45, loss = 0.03562591\n",
      "Iteration 46, loss = 0.03479303\n",
      "Iteration 47, loss = 0.03375737\n",
      "Iteration 48, loss = 0.03154861\n",
      "Iteration 49, loss = 0.03019101\n",
      "Iteration 50, loss = 0.02823606\n",
      "Iteration 51, loss = 0.02758440\n",
      "Iteration 52, loss = 0.02614853\n",
      "Iteration 53, loss = 0.02626517\n",
      "Iteration 54, loss = 0.02579586\n",
      "Iteration 55, loss = 0.02393966\n",
      "Iteration 56, loss = 0.02267040\n",
      "Iteration 57, loss = 0.02208961\n",
      "Iteration 58, loss = 0.02068097\n",
      "Iteration 59, loss = 0.02038668\n",
      "Iteration 60, loss = 0.01912625\n",
      "Iteration 61, loss = 0.01916609\n",
      "Iteration 62, loss = 0.01810985\n",
      "Iteration 63, loss = 0.01735256\n",
      "Iteration 64, loss = 0.01694601\n",
      "Iteration 65, loss = 0.01656195\n",
      "Iteration 66, loss = 0.01594867\n",
      "Iteration 67, loss = 0.01535156\n",
      "Iteration 68, loss = 0.01524899\n",
      "Iteration 69, loss = 0.01382694\n",
      "Iteration 70, loss = 0.01334672\n",
      "Iteration 71, loss = 0.01286830\n",
      "Iteration 72, loss = 0.01356191\n",
      "Iteration 73, loss = 0.01307152\n",
      "Iteration 74, loss = 0.01323329\n",
      "Iteration 75, loss = 0.01126398\n",
      "Iteration 76, loss = 0.01108027\n",
      "Iteration 77, loss = 0.01094208\n",
      "Iteration 78, loss = 0.01016498\n",
      "Iteration 79, loss = 0.01001575\n",
      "Iteration 80, loss = 0.00990580\n",
      "Iteration 81, loss = 0.00967503\n",
      "Iteration 82, loss = 0.00904018\n",
      "Iteration 83, loss = 0.00924199\n",
      "Iteration 84, loss = 0.00909305\n",
      "Iteration 85, loss = 0.00821385\n",
      "Iteration 86, loss = 0.00818983\n",
      "Iteration 87, loss = 0.00788099\n",
      "Iteration 88, loss = 0.00753628\n",
      "Iteration 89, loss = 0.00746164\n",
      "Iteration 90, loss = 0.00701766\n",
      "Iteration 91, loss = 0.00707296\n",
      "Iteration 92, loss = 0.00682670\n",
      "Iteration 93, loss = 0.00650030\n",
      "Iteration 94, loss = 0.00643900\n",
      "Iteration 95, loss = 0.00646426\n",
      "Iteration 96, loss = 0.00660606\n",
      "Iteration 97, loss = 0.00592238\n",
      "Iteration 98, loss = 0.00583823\n",
      "Iteration 99, loss = 0.00560744\n",
      "Iteration 100, loss = 0.00546817\n",
      "Iteration 101, loss = 0.00510973\n",
      "Iteration 102, loss = 0.00528740\n",
      "Iteration 103, loss = 0.00504475\n",
      "Iteration 104, loss = 0.00481755\n",
      "Iteration 105, loss = 0.00461950\n",
      "Iteration 106, loss = 0.00460684\n",
      "Iteration 107, loss = 0.00457059\n",
      "Iteration 108, loss = 0.00435628\n",
      "Iteration 109, loss = 0.00420198\n",
      "Iteration 110, loss = 0.00438127\n",
      "Iteration 111, loss = 0.00407932\n",
      "Iteration 112, loss = 0.00408670\n",
      "Iteration 113, loss = 0.00398482\n",
      "Iteration 114, loss = 0.00392582\n",
      "Iteration 115, loss = 0.00362586\n",
      "Iteration 116, loss = 0.00370390\n",
      "Iteration 117, loss = 0.00363059\n",
      "Iteration 118, loss = 0.00330668\n",
      "Iteration 119, loss = 0.00330028\n",
      "Iteration 120, loss = 0.00316854\n",
      "Iteration 121, loss = 0.00317240\n",
      "Iteration 122, loss = 0.00321108\n",
      "Iteration 123, loss = 0.00321356\n",
      "Iteration 124, loss = 0.00297405\n",
      "Iteration 125, loss = 0.00282902\n",
      "Iteration 126, loss = 0.00285982\n",
      "Iteration 127, loss = 0.00278305\n",
      "Iteration 128, loss = 0.00278450\n",
      "Iteration 129, loss = 0.00270021\n",
      "Iteration 130, loss = 0.00254213\n",
      "Iteration 131, loss = 0.00251897\n",
      "Iteration 132, loss = 0.00246383\n",
      "Iteration 133, loss = 0.00238484\n",
      "Iteration 134, loss = 0.00230901\n",
      "Iteration 135, loss = 0.00226750\n",
      "Iteration 136, loss = 0.00225275\n",
      "Iteration 137, loss = 0.00222786\n",
      "Iteration 138, loss = 0.00221759\n",
      "Iteration 139, loss = 0.00224378\n",
      "Iteration 140, loss = 0.00225355\n",
      "Iteration 141, loss = 0.00205587\n",
      "Iteration 142, loss = 0.00198568\n",
      "Iteration 143, loss = 0.00198740\n",
      "Iteration 144, loss = 0.00195719\n",
      "Iteration 145, loss = 0.00192595\n",
      "Iteration 146, loss = 0.00184712\n",
      "Iteration 147, loss = 0.00182948\n",
      "Iteration 148, loss = 0.00183168\n",
      "Iteration 149, loss = 0.00183653\n",
      "Iteration 150, loss = 0.00170844\n",
      "Iteration 151, loss = 0.00169768\n",
      "Iteration 152, loss = 0.00166450\n",
      "Iteration 153, loss = 0.00163117\n",
      "Iteration 154, loss = 0.00160051\n",
      "Iteration 155, loss = 0.00156814\n",
      "Iteration 156, loss = 0.00154661\n",
      "Iteration 157, loss = 0.00153753\n",
      "Iteration 158, loss = 0.00153584\n",
      "Iteration 159, loss = 0.00146342\n",
      "Iteration 160, loss = 0.00144027\n",
      "Iteration 161, loss = 0.00142020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and Train Simple Neural Network (MLP)\n",
    "mlp_simples = MLPClassifier(hidden_layer_sizes=(37, 37),\n",
    "                            activation='relu',\n",
    "                            solver='adam',\n",
    "                            batch_size=50,\n",
    "                            max_iter=1000,\n",
    "                            verbose=1)\n",
    "\n",
    "mlp_simples.fit(previsores_treinamento, classe_treinamento)\n",
    "\n",
    "# Make predictions using the simple MLP model\n",
    "previsoes_mlp = mlp_simples.predict(previsores_teste)\n",
    "\n",
    "# Evaluate the accuracy of the simple MLP model\n",
    "precisao_mlp = metrics.accuracy_score(previsoes_mlp, classe_teste)\n",
    "precisao_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions and Evaluate Simple MLP Model\n",
    "previsoes_mlp = mlp_simples.predict(previsores_teste)\n",
    "precisao_mlp = metrics.accuracy_score(previsoes_mlp, classe_teste)\n",
    "precisao_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions using the simple MLP model\n",
    "previsoes_mlp = mlp_simples.predict(previsores_teste)\n",
    "\n",
    "# Evaluate the accuracy of the simple MLP model\n",
    "precisao_mlp = metrics.accuracy_score(previsoes_mlp, classe_teste)\n",
    "precisao_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results\n",
    "Comparando os resultados, com RBM chegamos em 0.93 e sem RBM o percentual é de 0.98.\n",
    "\n",
    "Com isso chegamos a conclusão que usar RBM com essa base de dados e com redes neurais piora os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with RBM: 0.95\n",
      "Accuracy without RBM: 0.97\n",
      "Using RBM with this dataset and neural networks worsens the results.\n"
     ]
    }
   ],
   "source": [
    "# Compare Results\n",
    "\n",
    "# Compare the accuracy of the RBM + MLP model and the simple MLP model\n",
    "print(f\"Accuracy with RBM: {precisao_rbm:.2f}\")\n",
    "print(f\"Accuracy without RBM: {precisao_mlp:.2f}\")\n",
    "\n",
    "# Conclusion based on the comparison\n",
    "if precisao_rbm < precisao_mlp:\n",
    "    print(\"Using RBM with this dataset and neural networks worsens the results.\")\n",
    "else:\n",
    "    print(\"Using RBM with this dataset and neural networks improves the results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
